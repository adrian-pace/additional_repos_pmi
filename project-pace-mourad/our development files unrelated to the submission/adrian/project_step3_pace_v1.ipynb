{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.feature_reduction import *\n",
    "from src.classifiers import *\n",
    "from src.plots import *\n",
    "from src.CV import *\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as prepro\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157749248\n"
     ]
    }
   ],
   "source": [
    "random_state=0\n",
    "proc = psutil.Process(os.getpid())\n",
    "mem0 = proc.memory_info().rss\n",
    "print(mem0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_weight=2\n",
    "negation_distance_threshold_review=2\n",
    "negation_distance_threshold_summary=4\n",
    "sample_size=50000\n",
    "save_location='output_figs3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(path_or_buf='amazon_step23.json',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>5</td>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came ou...</td>\n",
       "      <td>09 12, 2006</td>\n",
       "      <td>A3EBHHCZO6V2A4</td>\n",
       "      <td>Amaranth \"music fan\"</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>1158019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>A clasically-styled and introverted album, Mem...</td>\n",
       "      <td>06 3, 2001</td>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>bethtexas</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>991526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>5</td>\n",
       "      <td>I never thought Enya would reach the sublime h...</td>\n",
       "      <td>07 14, 2003</td>\n",
       "      <td>A38IRL0X2T4DPF</td>\n",
       "      <td>bob turnley</td>\n",
       "      <td>The best so far</td>\n",
       "      <td>1058140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>This is the third review of an irish album I w...</td>\n",
       "      <td>05 3, 2000</td>\n",
       "      <td>A22IK3I6U76GX0</td>\n",
       "      <td>Calle</td>\n",
       "      <td>Ireland produces good music.</td>\n",
       "      <td>957312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5555991584</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>Enya, despite being a successful recording art...</td>\n",
       "      <td>01 17, 2008</td>\n",
       "      <td>A1AISPOIIHTHXX</td>\n",
       "      <td>Cloud \"...\"</td>\n",
       "      <td>4.5; music to dream to</td>\n",
       "      <td>1200528000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  5555991584  [3, 3]        5   \n",
       "1  5555991584  [0, 0]        5   \n",
       "2  5555991584  [2, 2]        5   \n",
       "3  5555991584  [1, 1]        5   \n",
       "4  5555991584  [1, 1]        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  It's hard to believe \"Memory of Trees\" came ou...  09 12, 2006   \n",
       "1  A clasically-styled and introverted album, Mem...   06 3, 2001   \n",
       "2  I never thought Enya would reach the sublime h...  07 14, 2003   \n",
       "3  This is the third review of an irish album I w...   05 3, 2000   \n",
       "4  Enya, despite being a successful recording art...  01 17, 2008   \n",
       "\n",
       "       reviewerID          reviewerName                       summary  \\\n",
       "0  A3EBHHCZO6V2A4  Amaranth \"music fan\"       Enya's last great album   \n",
       "1   AZPWAXJG9OJXV             bethtexas      Enya at her most elegant   \n",
       "2  A38IRL0X2T4DPF           bob turnley               The best so far   \n",
       "3  A22IK3I6U76GX0                 Calle  Ireland produces good music.   \n",
       "4  A1AISPOIIHTHXX           Cloud \"...\"        4.5; music to dream to   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1158019200  \n",
       "1       991526400  \n",
       "2      1058140800  \n",
       "3       957312000  \n",
       "4      1200528000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO remove sampling\n",
    "# Sampling to minimze computing cost\n",
    "df = df.sample(sample_size,random_state=random_state)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original test:  This is the bottom line: If you like New-Wave/Electronic music of the early eighties and you had no idea who A Flock Of Seagulls were or had no way of associating them with the way they looked or their MTV status, then you would like this album. A very undderrated band for sure, this album is awesome especially for (It's Not Me) Talking, Wishing, Nightmares, and The Traveller. I'll admit the bonus B-Sides arent the greatest but the live version of I Ran is excellent and the orginal album holds up very well. I dont think its my absolute favorite but it ties with their first album( DNA and Messages are two of my favs). The guitar and keyboard sound so good together it really creates a timeless feel, even for a New Wave band. This album speaks for itsself(and its era). Do as the album title implies...good cd. Nuff said \n",
      "\n",
      "summary test:  Still cool to listen to \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a corpus class with an iterator that reads one corpus document per line without loading all into memory\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import enchant\n",
    "\n",
    "eng_dic = enchant.Dict(\"en_US\")\n",
    "tester = 1\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "documents = np.array(df['reviewText'])\n",
    "\n",
    "#print ('original: ',documents[tester], '\\n')\n",
    "\n",
    "# TODO use the stopwords file ? should ask fayez\n",
    "\n",
    "negativeword_file=open('negative_words.txt')\n",
    "NEGATIVEWORDS=negativeword_file.read().split()\n",
    "\n",
    "positiveword_file=open('positive_words.txt')\n",
    "POSITIVEWORDS=positiveword_file.read().split()\n",
    "\n",
    "negationword_file=open('negation_words.txt')\n",
    "NEGATIONWORDS=negationword_file.read().split()\n",
    "\n",
    "# Not Merging both reviewText and summary because summary is more important\n",
    "#documents = df[['reviewText', 'summary']].apply(lambda x: ''.join(x), axis=1)\n",
    "documents=np.array(df['reviewText'])\n",
    "summaries=np.array(df['summary'])\n",
    "\n",
    "print ('original test: ',documents[tester], '\\n')\n",
    "print ('summary test: ',summaries[tester], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove special characters\n",
    "documents_no_specials=remove_specials_characters(documents)\n",
    "# remove stop words and tokenize\n",
    "documents_no_stop= []\n",
    "for document in documents_no_specials:\n",
    "    new_text=[]\n",
    "    for word in document.lower().split():\n",
    "        if word not in STOPWORDS:\n",
    "            new_text.append(word)\n",
    "    documents_no_stop.append(new_text)\n",
    "    \n",
    "#print ('tokenize and remove stop words: ',documents_no_stop[tester], '\\n')\n",
    "\n",
    "#Remove special characters\n",
    "summaries_no_specials=remove_specials_characters(summaries)\n",
    "# remove stop words and tokenize\n",
    "summaries_no_stop= []\n",
    "for document in summaries_no_specials:\n",
    "    new_text=[]\n",
    "    for word in document.lower().split():\n",
    "        if word not in STOPWORDS:\n",
    "            new_text.append(word)\n",
    "    summaries_no_stop.append(new_text)\n",
    "    \n",
    "#print ('tokenize and remove stop words: ',documents_no_stop[tester], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "documents_no_stop_no_numeric = remove_numerical(documents_no_stop)\n",
    "summaries_no_stop_no_numeric = remove_numerical(summaries_no_stop)\n",
    "#print ('remove numerics: ',documents_no_stop_no_numeric[tester], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmattizing tokens (better than stemming by taking word context into account)\n",
    "\n",
    "documents_no_stop_no_numeric_lemmatize = [[lemmatizer.lemmatize(token) for token in text] \n",
    "                                                    for text in documents_no_stop_no_numeric]\n",
    "summaries_no_stop_no_numeric_lemmatize = [[lemmatizer.lemmatize(token) for token in text] \n",
    "                                                    for text in summaries_no_stop_no_numeric]\n",
    "\n",
    "#print ('lemmatize: ',documents_no_stop_no_numeric_lemmatize[tester], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no english:  ['line', 'like', 'new', 'wave', 'electronic', 'music', 'early', 'eighty', 'idea', 'flock', 'seagull', 'way', 'associating', 'way', 'looked', 'status', 'like', 'album', 'band', 'sure', 'album', 'awesome', 'especially', 'talking', 'wishing', 'nightmare', 'ill', 'admit', 'bonus', 'b', 'side', 'greatest', 'live', 'version', 'ran', 'excellent', 'album', 'hold', 'think', 'absolute', 'favorite', 'tie', 'album', 'message', 'guitar', 'keyboard', 'sound', 'good', 'creates', 'timeless', 'feel', 'new', 'wave', 'band', 'album', 'speaks', 'era', 'album', 'title', 'implies', 'good', 'said'] \n",
      "\n",
      "no english:  ['cool', 'listen'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove non-english words\n",
    "\n",
    "documents_no_stop_no_numeric_lemmatize_english = [[token for token in text if (eng_dic.check(token)) ] \n",
    "                                                            for text in documents_no_stop_no_numeric_lemmatize]\n",
    "\n",
    "print ('no english: ',documents_no_stop_no_numeric_lemmatize_english[tester], '\\n')\n",
    "\n",
    "summaries_no_stop_no_numeric_lemmatize_english = [[token for token in text if (eng_dic.check(token)) ] \n",
    "                                                            for text in summaries_no_stop_no_numeric_lemmatize]\n",
    "\n",
    "print ('no english: ',summaries_no_stop_no_numeric_lemmatize_english[tester], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create ready corpus\n",
    "\n",
    "df['reviewCleaned'] = [\" \".join(doc) for doc in documents_no_stop_no_numeric_lemmatize_english] \n",
    "df['summaryCleaned'] = [\" \".join(doc) for doc in summaries_no_stop_no_numeric_lemmatize_english] \n",
    "\n",
    "#df['reviewCleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d45fe341b82c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# the product a juste write 'good...' would get an extremely high ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m documents_neg_pos, documents_pos_count, documents_neg_count = keepAdjectives(\n\u001b[0;32m---> 46\u001b[0;31m     df['reviewCleaned'],NEGATIONWORDS,POSITIVEWORDS,NEGATIVEWORDS,negation_distance_threshold_review)\n\u001b[0m\u001b[1;32m     47\u001b[0m print(\"Kept only adjectives pairs:\",documents_neg_pos[tester], \n\u001b[1;32m     48\u001b[0m       '\\nPositive count=',documents_pos_count[tester],'Negative counts=',documents_neg_count[tester])\n",
      "\u001b[0;32m<ipython-input-13-d45fe341b82c>\u001b[0m in \u001b[0;36mkeepAdjectives\u001b[0;34m(documents, NEGATIONWORDS, POSITIVEWORDS, NEGATIVEWORDS, negation_distance_threshold)\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNEGATIVEWORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def keepAdjectives(documents,NEGATIONWORDS, POSITIVEWORDS, NEGATIVEWORDS,negation_distance_threshold):\n",
    "    \n",
    "    document_neg_pos = []\n",
    "    document_neg_count = np.zeros(df.shape[0])\n",
    "    document_pos_count = np.zeros(df.shape[0])\n",
    "\n",
    "    for idx,row in enumerate(documents):\n",
    "        review = []\n",
    "        new_word = \"\"\n",
    "        negation_distance=0\n",
    "        \n",
    "        for word in row.split(\" \"):\n",
    "            if len(new_word) != 1:\n",
    "                negation_distance+=1\n",
    "                if negation_distance>negation_distance_threshold:\n",
    "                    new_word=\"\"\n",
    "            if word in NEGATIONWORDS:\n",
    "                new_word = word\n",
    "                negation_distance=0\n",
    "            elif word in POSITIVEWORDS:\n",
    "                if len(new_word)==0:\n",
    "                    review.append(word)\n",
    "                    document_pos_count[idx]+=1\n",
    "                else:\n",
    "                    #print(idx)\n",
    "                    document_neg_count[idx]+=1\n",
    "                    new_word = new_word + word\n",
    "                    review.append(new_word)\n",
    "                    new_word = \"\"\n",
    "            elif word in NEGATIVEWORDS:\n",
    "                if len(new_word)==0:\n",
    "                    review.append(word)\n",
    "                    document_neg_count[idx]+=1\n",
    "                else:\n",
    "                    #print(idx)\n",
    "                    document_pos_count[idx]+=1\n",
    "                    new_word = new_word + word\n",
    "                    review.append(new_word)\n",
    "                    new_word = \"\"\n",
    "        document_neg_pos.append(review)\n",
    "    return document_neg_pos, document_pos_count, document_neg_count\n",
    "\n",
    "# Not sure if ratio of counts is relevent since someone who like average\n",
    "# the product a juste write 'good...' would get an extremely high ratio\n",
    "documents_neg_pos, documents_pos_count, documents_neg_count = keepAdjectives(\n",
    "    df['reviewCleaned'],NEGATIONWORDS,POSITIVEWORDS,NEGATIVEWORDS,negation_distance_threshold_review)\n",
    "print(\"Kept only adjectives pairs:\",documents_neg_pos[tester], \n",
    "      '\\nPositive count=',documents_pos_count[tester],'Negative counts=',documents_neg_count[tester])\n",
    "\n",
    "summaries_neg_pos, summaries_pos_count, summaries_neg_count = keepAdjectives(\n",
    "    df['summaryCleaned'],NEGATIONWORDS,POSITIVEWORDS,NEGATIVEWORDS,negation_distance_threshold_summary)\n",
    "print(\"Kept only adjectives pairs:\",summaries_neg_pos[tester], \n",
    "      '\\nPositive count=',summaries_pos_count[tester],'Negative counts=',summaries_neg_count[tester])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('original text: ',documents[373], '\\n')\n",
    "print ('no english: ',documents_no_stop_no_numeric_lemmatize_english[373], '\\n')\n",
    "print(\"Kept only adjectives pairs:\",documents_neg_pos[373], \n",
    "      '\\nPositive count=',documents_pos_count[373],'Negative counts=',documents_neg_count[373])\n",
    "print ('original text: ',summaries[373], '\\n')\n",
    "print ('no english: ',summaries_no_stop_no_numeric_lemmatize_english[373], '\\n')\n",
    "print(\"Kept only adjectives pairs:\",summaries_neg_pos[373],\n",
    "      '\\nPositive count=',summaries_pos_count[373],'Negative counts=',summaries_neg_count[373])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"reviewWorded\"] = [\" \".join(doc) for doc in documents_neg_pos]\n",
    "df[\"summaryWorded\"] = [\" \".join(doc) for doc in summaries_neg_pos]\n",
    "df['review_count_pos'] = documents_pos_count\n",
    "df['review_count_neg'] = documents_neg_count\n",
    "df['summary_count_pos'] = summaries_pos_count\n",
    "df['summary_count_neg'] = summaries_neg_count\n",
    "# Not sure if ratio of counts is relevent since someone who like average\n",
    "# the product a juste write 'good...' would get an extremely high ratio.\n",
    "# However, the difference between the two could be interesting\n",
    "df['review_count_difference']=df['review_count_pos']-df['review_count_neg']\n",
    "df['summary_count_difference']=df['summary_count_pos']-df['summary_count_neg']\n",
    "df['total_count_difference']=df['review_count_difference']+summary_weight*df['summary_count_difference']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REMOVE DF_min and df _ max\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorizer = TfidfVectorizer() We don't need to reduce the importance of words that appear often since they are already filtered\n",
    "\n",
    "# fit vectorizer, carry out vectorization and display results\n",
    "vectorizer.fit(df[['reviewWorded', 'summaryWorded']].apply(lambda x: ''.join(x), axis=1))   # TODO join with df[['reviewText', 'summary']].apply(lambda x: ''.join(x), axis=1)\n",
    "documents_vec = vectorizer.transform(df['reviewWorded'])\n",
    "summaries_vec = vectorizer.transform(df['summaryWorded'])\n",
    "summaries_vec*=summary_weight\n",
    "documents_vec+=summaries_vec\n",
    "#print(documents_vec) # sparse matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(documents_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "mem1 = proc.memory_info().rss\n",
    "del lemmatizer, document,documents,documents_no_specials,documents_no_stop,documents_no_stop_no_numeric\n",
    "del documents_no_stop_no_numeric_lemmatize,documents_no_stop_no_numeric_lemmatize_english, eng_dic\n",
    "del  summaries,summaries_no_specials,summaries_no_stop,summaries_no_stop_no_numeric\n",
    "del summaries_no_stop_no_numeric_lemmatize,summaries_no_stop_no_numeric_lemmatize_english\n",
    "mem2 = proc.memory_info().rss\n",
    "gc.collect()\n",
    "mem3 = proc.memory_info().rss\n",
    "pdf = lambda x2, x1: 100.0 * (x2 - x1) / mem0\n",
    "print (\"Allocation: %0.2f%%\" % pdf(mem1, mem0))\n",
    "print (\"Unreference: %0.2f%%\" % pdf(mem2, mem1))\n",
    "print (\"Collect: %0.2f%%\" % pdf(mem3, mem2))\n",
    "print (\"Overall: %0.2f%%\" % pdf(mem3, mem0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is the review word vector. y are the corresponding categories.\n",
    "\n",
    "The data is Min/max scaled into [0;1]. It isn't standartize because we want positive values. One could try a normalization (l1 or l2 for example)\n",
    "\n",
    "We split the data into a train and test set. \n",
    "\n",
    "tf/idf is not relevant since words that appear often might suggest a category (such as 'book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=df['overall']\n",
    "ranks=np.unique(y)\n",
    "X_scaled=preprocessing.normalize(documents_vec,norm='max', copy=False)\n",
    "X=df[['total_count_difference','review_count_pos','review_count_neg','summary_count_pos','summary_count_neg']]\n",
    "\n",
    "proc.memory_info().rss\n",
    "\n",
    "from scipy.sparse import hstack,csr_matrix\n",
    "count_diff = csr_matrix(df['total_count_difference']).T\n",
    "count_pos_rev = csr_matrix(df['review_count_pos']).T\n",
    "count_neg_rev = csr_matrix(df['review_count_neg']).T\n",
    "count_pos_sum = csr_matrix(df['summary_count_pos']).T\n",
    "count_neg_sum = csr_matrix(df['summary_count_neg']).T\n",
    "\n",
    "hstack([count_neg_sum,X_scaled])\n",
    "X=hstack([hstack([hstack([count_diff,count_pos_rev]),hstack([count_neg_rev,count_pos_sum])])\n",
    "         ,hstack([count_neg_sum,X_scaled])])\n",
    "\n",
    "\n",
    "#X=X_scaled # without counts\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proc.memory_info().rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy arrays\n",
    "#X_train=np.array(X_train)\n",
    "#X_test=np.array(X_test)\n",
    "#y_train=np.array(y_train)\n",
    "#y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select k best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0cb433215af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m parameter_search_feature_selection(LinearRegression(n_jobs=-1),SelectKBest(),'k',np.logspace(2,3.7,10).astype(int),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                    X_train,y_train,random_state=random_state,save_file=save_location+'/slt_k_best.png')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "parameter_search_feature_selection(LinearRegression(n_jobs=-1),SelectKBest(),'k',np.logspace(2,3.7,10).astype(int),\n",
    "                                   X_train,y_train,random_state=random_state,save_file=save_location+'/slt_k_best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_concepts,X_test_concepts = LSI_concepts(X_train,X_test,num_lsi_topics = 100,words=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.417\n",
      "Execution time : 15.908146142959595 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train,categories)\n",
    "print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.297\n",
      "30\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.298\n",
      "35\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.31\n",
      "40\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.323\n",
      "50\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.345\n",
      "60\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.376\n",
      "70\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.385\n",
      "80\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.393\n",
      "90\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.424\n",
      "100\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.424\n",
      "200\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.465\n",
      "300\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.477\n",
      "400\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.489\n",
      "500\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.494\n",
      "600\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.501\n",
      "700\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.502\n",
      "800\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.502\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.507\n",
      "1000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.504\n",
      "2000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.513\n",
      "3000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.514\n",
      "4000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.514\n",
      "5000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.514\n",
      "6000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.514\n",
      "7000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "accuracy: 0.514\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea7c5137b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time : 3555.529062271118 seconds\n",
      "best num_lsi_topics= 3000 with accuracy score: 0.514\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "accuracy=[]\n",
    "num_lsi_topics_array = [25,30,35,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,6000,7000]\n",
    "# Check various C\n",
    "for num_lsi_topics in num_lsi_topics_array:\n",
    "    print(num_lsi_topics)\n",
    "    # fit classifier \n",
    "    X_train_concepts,X_test_concepts = LSI_concepts(X_train,X_test,num_lsi_topics,words=vectorizer.get_feature_names())\n",
    "    \n",
    "    y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train, categories)\n",
    "    print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    accuracy.append(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# Plot\n",
    "plt.plot(num_lsi_topics_array,accuracy)\n",
    "plt.ylabel(\"accuracy Score\")\n",
    "plt.xlabel(\"number of topics values\")\n",
    "plt.show()\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))\n",
    "best_c=num_lsi_topics_array[np.argmax(accuracy)]\n",
    "print(\"best num_lsi_topics=\",best_c,\"with accuracy score:\", np.max(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d65a4f5df579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# USE LSI number of concepts and then tune update every and passes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_concepts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_concepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA_concepts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_lda_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_concepts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_concepts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pace/Documents/data_science/project-pace-mourad/adrian/feature_reduction.py\u001b[0m in \u001b[0;36mLDA_concepts\u001b[0;34m(X_train, X_test, num_lda_topics, words, update_every, passes)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense2Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     lsi_transformer = lda_transformer = models.LdaModel(corpus, id2word=dictionary, num_topics=num_lda_topics,\n\u001b[0;32m---> 46\u001b[0;31m                                                         update_every=update_every, passes=passes, alpha='auto')\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mcorpus_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_transformer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mX_train_concepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_lda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_lda_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pace/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# if a training corpus was provided, start estimating the model right away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pace/anaconda3/lib/python3.6/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mdirichlet_expectation\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# keep the same precision as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# USE LSI number of concepts and then tune update every and passes\n",
    "start = time.time()\n",
    "X_train_concepts,X_test_concepts = LDA_concepts(X_train,X_test,num_lda_topics = 100,words=vectorizer.get_feature_names())\n",
    "y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train,categories)\n",
    "print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "accuracy=[]\n",
    "num_lda_topics_array = [25,30,35,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,6000,7000]\n",
    "# Check various C\n",
    "for num_lda_topics in num_lda_topics_array:\n",
    "    print(num_lda_topics)\n",
    "    # fit classifier \n",
    "    X_train_concepts,X_test_concepts = LDA_concepts(X_train,X_test,num_lsi_topics,words=vectorizer.get_feature_names())\n",
    "    \n",
    "    y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train, categories)\n",
    "    print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    accuracy.append(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# Plot\n",
    "plt.plot(num_lda_topics_array,accuracy)\n",
    "plt.ylabel(\"accuracy Score\")\n",
    "plt.xlabel(\"Number of topics values\")\n",
    "plt.show()\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))\n",
    "best_c=num_lsi_topics_array[np.argmax(accuracy)]\n",
    "print(\"best num_lsi_topics=\",best_c,\"with accuracy score:\", np.max(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dummy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model: Assign the categories randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error: 0.853789825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "dmy=DummyRegressor()\n",
    "dmy.fit(X_train,y_train)\n",
    "y_pred=dmy.predict(X_test)\n",
    "print(\"mean_absolute_error:\",mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "parameter_search(Ridge(),\n",
    "                {'alpha':np.logspace(-5,5,40)},\n",
    "                X_train,\n",
    "                np.array(y_train), save_file=save_location+'/ridge.png',scoring='neg_mean_absolute_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 22110)\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  52.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  56.7s\n",
      "[CV] ................................................. , total=  58.9s\n",
      "[CV] ................................................. , total=  59.1s\n",
      "[CV] ................................................. , total=  19.2s\n",
      "Mean CV valdiation score: -0.724936871698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print (X_train.shape)\n",
    "ls=LinearRegression()\n",
    "print(\"Mean CV valdiation score:\",np.mean(cross_val_score(\n",
    "    ls,X_train,y_train,scoring='neg_mean_absolute_error',cv=5,verbose=2,n_jobs=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-0.8 neg_MAE with 50000 just counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-0.724936871698 neg_MAE with 50000 all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " -0.730031544571 neg_MAE with 50000 only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "ls=LinearRegression()\n",
    "ls.fit(X_train,y_train)\n",
    "y_pred=ls.predict(X_test)\n",
    "print(mean_absolute_error(y_test,y_pred))\n",
    "y_pred=np.round(y_pred)\n",
    "y_pred=np.where(y_pred>5,5,y_pred)\n",
    "y_pred=np.where(y_pred<1,1,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
