{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.feature_reduction import *\n",
    "from src.classifiers import *\n",
    "from src.plots import *\n",
    "from src.CV import *\n",
    "\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as prepro\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',-1)\n",
    "df = pd.read_json(path_or_buf='amazon_step23.json',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64706, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewCleaned</th>\n",
       "      <th>reviewWorded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51597</th>\n",
       "      <td>B000E0OEQC</td>\n",
       "      <td>[34, 34]</td>\n",
       "      <td>4</td>\n",
       "      <td>So I was going to buy this album around the time it first came out, but I was a bit reluctant for some reason or another. Then I looked on Amazon and noticed all the positive reviews. I looked online and this album has sold over a quarter million with absolutely no promotion or support from the label. (Until just recently when MTV and BET have taken notice). I figured that if this guy has such a following, it has to be good. (He also has quite a following with other artists. Pharrell executive produced it, Lil' Wayne's here, Faith Evans is here, Mary J. Blige demanded to do a duet with him... wow.) So I ordered it.Blue eyed soul has been attempted numerous times, but this album is different. This album keeps the integrity of soul music and it has credibility. It is authentic.I loved this album from the second I started listening. The opening track with Faith Evans, \"Got 2 Be Down,\" was one of my favorites. \"Ask Myself\" is excellent. (Thicke and Mary J. Blige re-recorded a duet version of this song for the Mary J. Blige &amp; Friends compilation. It's quite nice.) \"Shooter\" with Lil' Wayne is amazingly innovative and I just love it.The last three songs on this album are also beautiful and have great lyrics and messages.The production, mostly done by Robin himself and Pro J, is excellent. The lyircs are nice and his voice is great. (And no, this abum isn't completely falsetto.) The only real problem is that the album runs a little too long. I highly recommend that you buy this album.The deluxe edition comes with a ringtone, cell phone wallpaper and \"autographed\" poster. If you have the option, just buy the original because it's cheaper and this is a sad excuse for a \"deluxe\" edition...</td>\n",
       "      <td>02 11, 2007</td>\n",
       "      <td>A1S2IY37JU93XS</td>\n",
       "      <td>W. E. Phillips</td>\n",
       "      <td>...</td>\n",
       "      <td>1171152000</td>\n",
       "      <td>going buy album around time first came bit reluctant reason another looked amazon noticed positive review looked album sold quarter million absolutely promotion support label just recently bet taken notice figured guy following good also quite following artist executive produced faith j demanded duet wow ordered blue eyed soul attempted numerous time album different album keep integrity soul music credibility authentic loved album second started listening opening track faith got one favorite ask excellent j re recorded duet version song j friend compilation quite nice shooter amazingly innovative just love last three song album also beautiful great lyric message production mostly done robin pro j excellent nice voice great completely falsetto real problem album run little long highly recommend buy album deluxe edition come cell phone wallpaper autographed poster option just buy original cheaper sad excuse deluxe edition</td>\n",
       "      <td>reluctant positive support good faith wow authentic loved faith favorite excellent nice amazingly innovative love beautiful great excellent nice great problem recommend cheaper sad excuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18021</th>\n",
       "      <td>B000002NAA</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>Yeah, it's a good 'un.  I'd rank it up there with the top VH cd's.  I think you'll dig it.</td>\n",
       "      <td>07 9, 2002</td>\n",
       "      <td>A1NVEW628JUS2Y</td>\n",
       "      <td>Da Peace Dogg</td>\n",
       "      <td>Good stuff</td>\n",
       "      <td>1026172800</td>\n",
       "      <td>yeah good id rank top think dig good stuff</td>\n",
       "      <td>good top good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3439</th>\n",
       "      <td>B000001AKT</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's what you get:1 CD of Diana Ross + Supremes hit material, mostly in mono, and sounding good. Nice stuff, and anyone who likes Ross clearly wants this material. Whether they need to get it on this box set (rather than a Supremes collection) is open to debate. In fact, anyone who buys this box probably already has most/all of this material on a Supremes collection as well.2 CDs of peak era Diana Ross solo, mostly drawn from her successful Motown era, but ending with some material from her decidedly less successful (both commercially and artistically) RCA albums.1 CD of material drawn from her (let's face it) pretty dire recent years. This material was unsuccessful in the marketplace, and deservedly so for the most part.Ross had a big hand in compiling this box, and that's not a positive thing. It's the reason we have the lopsided distribution of material. The late era material was overemphasized in an attempt to paint Ross (at the time this box was released) as a still-viable artist. The hard truth is that this material just drives home the point that wasn't.Another quibble would include the use of certain abridged single mixes of songs best heard in their extended glory.For the vast majority of people, a better listening experience could be had at lower cost by purchasing one of the many Supremes collections (in place of disc 1), and one of the several 2CD Diana Ross compilations devoted to her Motown era material. You don't get the fawning longbox sized booklet, but that's probably the only really substantial loss.</td>\n",
       "      <td>01 4, 2012</td>\n",
       "      <td>A2I18AO597DSDI</td>\n",
       "      <td>David Pearlman \"sound fanatic\"</td>\n",
       "      <td>Hard to believe this botched set is still in print...</td>\n",
       "      <td>1325635200</td>\n",
       "      <td>get supremes hit material mostly mono sounding good nice stuff anyone like clearly want material whether need get box set rather supremes collection open debate fact anyone buy box probably already material supremes collection well peak era solo mostly drawn successful era ending material decidedly successful commercially artistically album material drawn let face pretty dire recent year material unsuccessful marketplace deservedly part big hand compiling box positive thing reason lopsided distribution material late era material overemphasized attempt paint time box released still viable artist hard truth material just drive home point another quibble include use certain abridged single mix song best heard extended glory vast majority people better listening experience lower cost purchasing one many supremes collection place disc one several compilation devoted era material get fawning sized booklet probably really substantial loss hard believe botched set still print</td>\n",
       "      <td>good nice like clearly well successful successful pretty dire unsuccessful deservedly positive hard quibble best glory better loss hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27725</th>\n",
       "      <td>B00000G3X4</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>Anyonw who gives this a fair review must be a family meber of Clue or Clue himself! Who on earth does he think he is? If shouting out your name is all it takes to be a DJ, I guess I have it made! Apart from getting some pretty slick songs and putting them together in a nice neat package, he is the worst person in the music industry (even below Max Martin, and I hate Max Martin...). I don't understand why he has to shout his name at the beginning of the song. Anyone who has heard and of the rare songs he seems to get will not tell you that they really love the part when he shouts out his name at the beginning of the song. an even better question would be what the F@#K he says at the end (my cousin an i debate that it is either him saying 'ENOUGH!ENOUGH!ENOUGH!' or 'BLAH!BLAH!BLAH!'). Anyway, leave this alone. Anyone who has not heard anything by him is lucky. Those of us who have will never recover.</td>\n",
       "      <td>01 4, 2002</td>\n",
       "      <td>A19RTWM1F4ZU89</td>\n",
       "      <td>Donovan Juan</td>\n",
       "      <td>DJ CRAP!</td>\n",
       "      <td>1010102400</td>\n",
       "      <td>give fair review must family clue clue earth think shouting name take guess made apart getting pretty slick song putting together nice neat package worst person music industry even max martin hate max martin understand shout name beginning song anyone heard rare song seems get will tell really love part shout name beginning song even better question k say end cousin debate either saying enough enough enough blah blah blah anyway leave alone anyone heard anything lucky u will never recover crap</td>\n",
       "      <td>fair pretty slick nice neat worst hate love better enough enough enough blah blah blah lucky neverrecover crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44715</th>\n",
       "      <td>B0000UJLJG</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>It's more than decent. Not half as good as \"Bow Down\" but when you compare it to rap these days, I bet all the people calling this album wack back in 2003 feel stupid. The production is definitely lacking, but the rhymes are good. Especially Cube, WC is still dope, but Mack 10 kind of fell of with his skills.Best songs.Call 9-1-1.Get Ignit (the title suggest that it's just a stupid song resembling current rap, but it's actually a good song).Lights Out.Izm.Gangsta Nation (as with any album, you can't expect the lead single to be the best song, but this is decent).Potential Victims.So Many Rappers In Love (Funny, and to some people saying they're hypocrits for dissing love-song rappers.....there is a huge difference between having a loving spouse and children and rapping about it, what do you expect them to be single forever just so you'll think they're hard. Newsflash, Cube's been with his wife since he was in N.W.A.)</td>\n",
       "      <td>05 23, 2011</td>\n",
       "      <td>A2J3ZIX51LM84K</td>\n",
       "      <td>Justin Mitchell</td>\n",
       "      <td>I like it........but that's me.</td>\n",
       "      <td>1306108800</td>\n",
       "      <td>decent half good bow compare rap day bet people calling album back feel stupid production definitely lacking rhyme good especially cube still dope mack kind fell skill best song call get title suggest just stupid song resembling current rap actually good song light nation album cant expect lead single best song decent potential victim many rapper love funny people saying dissing love song rapper huge difference loving spouse child rapping expect single forever just think hard newsflash cube wife since n w like</td>\n",
       "      <td>decent good stupid lacking good dope fell skill best stupid good lead best decent love funny dissing love loving hard like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin   helpful  overall  \\\n",
       "51597  B000E0OEQC  [34, 34]  4         \n",
       "18021  B000002NAA  [0, 0]    4         \n",
       "3439   B000001AKT  [2, 2]    3         \n",
       "27725  B00000G3X4  [2, 3]    1         \n",
       "44715  B0000UJLJG  [0, 0]    3         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         reviewText  \\\n",
       "51597  So I was going to buy this album around the time it first came out, but I was a bit reluctant for some reason or another. Then I looked on Amazon and noticed all the positive reviews. I looked online and this album has sold over a quarter million with absolutely no promotion or support from the label. (Until just recently when MTV and BET have taken notice). I figured that if this guy has such a following, it has to be good. (He also has quite a following with other artists. Pharrell executive produced it, Lil' Wayne's here, Faith Evans is here, Mary J. Blige demanded to do a duet with him... wow.) So I ordered it.Blue eyed soul has been attempted numerous times, but this album is different. This album keeps the integrity of soul music and it has credibility. It is authentic.I loved this album from the second I started listening. The opening track with Faith Evans, \"Got 2 Be Down,\" was one of my favorites. \"Ask Myself\" is excellent. (Thicke and Mary J. Blige re-recorded a duet version of this song for the Mary J. Blige & Friends compilation. It's quite nice.) \"Shooter\" with Lil' Wayne is amazingly innovative and I just love it.The last three songs on this album are also beautiful and have great lyrics and messages.The production, mostly done by Robin himself and Pro J, is excellent. The lyircs are nice and his voice is great. (And no, this abum isn't completely falsetto.) The only real problem is that the album runs a little too long. I highly recommend that you buy this album.The deluxe edition comes with a ringtone, cell phone wallpaper and \"autographed\" poster. If you have the option, just buy the original because it's cheaper and this is a sad excuse for a \"deluxe\" edition...   \n",
       "18021  Yeah, it's a good 'un.  I'd rank it up there with the top VH cd's.  I think you'll dig it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3439   Here's what you get:1 CD of Diana Ross + Supremes hit material, mostly in mono, and sounding good. Nice stuff, and anyone who likes Ross clearly wants this material. Whether they need to get it on this box set (rather than a Supremes collection) is open to debate. In fact, anyone who buys this box probably already has most/all of this material on a Supremes collection as well.2 CDs of peak era Diana Ross solo, mostly drawn from her successful Motown era, but ending with some material from her decidedly less successful (both commercially and artistically) RCA albums.1 CD of material drawn from her (let's face it) pretty dire recent years. This material was unsuccessful in the marketplace, and deservedly so for the most part.Ross had a big hand in compiling this box, and that's not a positive thing. It's the reason we have the lopsided distribution of material. The late era material was overemphasized in an attempt to paint Ross (at the time this box was released) as a still-viable artist. The hard truth is that this material just drives home the point that wasn't.Another quibble would include the use of certain abridged single mixes of songs best heard in their extended glory.For the vast majority of people, a better listening experience could be had at lower cost by purchasing one of the many Supremes collections (in place of disc 1), and one of the several 2CD Diana Ross compilations devoted to her Motown era material. You don't get the fawning longbox sized booklet, but that's probably the only really substantial loss.                                                                                                                                                                     \n",
       "27725  Anyonw who gives this a fair review must be a family meber of Clue or Clue himself! Who on earth does he think he is? If shouting out your name is all it takes to be a DJ, I guess I have it made! Apart from getting some pretty slick songs and putting them together in a nice neat package, he is the worst person in the music industry (even below Max Martin, and I hate Max Martin...). I don't understand why he has to shout his name at the beginning of the song. Anyone who has heard and of the rare songs he seems to get will not tell you that they really love the part when he shouts out his name at the beginning of the song. an even better question would be what the F@#K he says at the end (my cousin an i debate that it is either him saying 'ENOUGH!ENOUGH!ENOUGH!' or 'BLAH!BLAH!BLAH!'). Anyway, leave this alone. Anyone who has not heard anything by him is lucky. Those of us who have will never recover.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "44715  It's more than decent. Not half as good as \"Bow Down\" but when you compare it to rap these days, I bet all the people calling this album wack back in 2003 feel stupid. The production is definitely lacking, but the rhymes are good. Especially Cube, WC is still dope, but Mack 10 kind of fell of with his skills.Best songs.Call 9-1-1.Get Ignit (the title suggest that it's just a stupid song resembling current rap, but it's actually a good song).Lights Out.Izm.Gangsta Nation (as with any album, you can't expect the lead single to be the best song, but this is decent).Potential Victims.So Many Rappers In Love (Funny, and to some people saying they're hypocrits for dissing love-song rappers.....there is a huge difference between having a loving spouse and children and rapping about it, what do you expect them to be single forever just so you'll think they're hard. Newsflash, Cube's been with his wife since he was in N.W.A.)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "        reviewTime      reviewerID                    reviewerName  \\\n",
       "51597  02 11, 2007  A1S2IY37JU93XS  W. E. Phillips                   \n",
       "18021  07 9, 2002   A1NVEW628JUS2Y  Da Peace Dogg                    \n",
       "3439   01 4, 2012   A2I18AO597DSDI  David Pearlman \"sound fanatic\"   \n",
       "27725  01 4, 2002   A19RTWM1F4ZU89  Donovan Juan                     \n",
       "44715  05 23, 2011  A2J3ZIX51LM84K  Justin Mitchell                  \n",
       "\n",
       "                                                     summary  unixReviewTime  \\\n",
       "51597  ...                                                    1171152000       \n",
       "18021  Good stuff                                             1026172800       \n",
       "3439   Hard to believe this botched set is still in print...  1325635200       \n",
       "27725  DJ CRAP!                                               1010102400       \n",
       "44715  I like it........but that's me.                        1306108800       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reviewCleaned  \\\n",
       "51597  going buy album around time first came bit reluctant reason another looked amazon noticed positive review looked album sold quarter million absolutely promotion support label just recently bet taken notice figured guy following good also quite following artist executive produced faith j demanded duet wow ordered blue eyed soul attempted numerous time album different album keep integrity soul music credibility authentic loved album second started listening opening track faith got one favorite ask excellent j re recorded duet version song j friend compilation quite nice shooter amazingly innovative just love last three song album also beautiful great lyric message production mostly done robin pro j excellent nice voice great completely falsetto real problem album run little long highly recommend buy album deluxe edition come cell phone wallpaper autographed poster option just buy original cheaper sad excuse deluxe edition                                                    \n",
       "18021  yeah good id rank top think dig good stuff                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3439   get supremes hit material mostly mono sounding good nice stuff anyone like clearly want material whether need get box set rather supremes collection open debate fact anyone buy box probably already material supremes collection well peak era solo mostly drawn successful era ending material decidedly successful commercially artistically album material drawn let face pretty dire recent year material unsuccessful marketplace deservedly part big hand compiling box positive thing reason lopsided distribution material late era material overemphasized attempt paint time box released still viable artist hard truth material just drive home point another quibble include use certain abridged single mix song best heard extended glory vast majority people better listening experience lower cost purchasing one many supremes collection place disc one several compilation devoted era material get fawning sized booklet probably really substantial loss hard believe botched set still print   \n",
       "27725  give fair review must family clue clue earth think shouting name take guess made apart getting pretty slick song putting together nice neat package worst person music industry even max martin hate max martin understand shout name beginning song anyone heard rare song seems get will tell really love part shout name beginning song even better question k say end cousin debate either saying enough enough enough blah blah blah anyway leave alone anyone heard anything lucky u will never recover crap                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "44715  decent half good bow compare rap day bet people calling album back feel stupid production definitely lacking rhyme good especially cube still dope mack kind fell skill best song call get title suggest just stupid song resembling current rap actually good song light nation album cant expect lead single best song decent potential victim many rapper love funny people saying dissing love song rapper huge difference loving spouse child rapping expect single forever just think hard newsflash cube wife since n w like                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "\n",
       "                                                                                                                                                                                      reviewWorded  \n",
       "51597  reluctant positive support good faith wow authentic loved faith favorite excellent nice amazingly innovative love beautiful great excellent nice great problem recommend cheaper sad excuse  \n",
       "18021  good top good                                                                                                                                                                                \n",
       "3439   good nice like clearly well successful successful pretty dire unsuccessful deservedly positive hard quibble best glory better loss hard                                                      \n",
       "27725  fair pretty slick nice neat worst hate love better enough enough enough blah blah blah lucky neverrecover crap                                                                               \n",
       "44715  decent good stupid lacking good dope fell skill best stupid good lead best decent love funny dissing love loving hard like                                                                   "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO remove sampling\n",
    "# Sampling to minimze computing cost\n",
    "df = df.sample(5000,random_state=random_state)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "print (a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3a925fa037d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#Merge both reviewText and summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewText'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#print ('original: ',documents[47316], '\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# create a corpus class with an iterator that reads one corpus document per line without loading all into memory\n",
    "from gensim import corpora\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import enchant\n",
    "\n",
    "eng_dic = enchant.Dict(\"en_US\")\n",
    "tester = 1\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Changed the stopwords from the list of step1.\n",
    "stopword_file=open('stopwords_list_step2.txt')\n",
    "STOPWORDS=stopword_file.read().split()\n",
    "\n",
    "negativeword_file=open('negative_words.txt')\n",
    "NEGATIVEWORDS=negativeword_file.read().split()\n",
    "\n",
    "positiveword_file=open('positive_words.txt')\n",
    "POSITIVEWORDS=positiveword_file.read().split()\n",
    "\n",
    "negationword_file=open('negation_words.txt')\n",
    "NEGATIONWORDS=negationword_file.read().split()\n",
    "\n",
    "#Merge both reviewText and summary\n",
    "documents = df[['reviewText', 'summary']].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "#print ('original: ',documents[47316], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special characters\n",
    "documents_no_specials=remove_specials_characters(documents)\n",
    "# remove stop words and tokenize\n",
    "documents_no_stop= []\n",
    "for document in documents_no_specials:\n",
    "    new_text=[]\n",
    "    for word in document.lower().split():\n",
    "        if word not in STOPWORDS:\n",
    "            new_text.append(word)\n",
    "    documents_no_stop.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT SURE IF WE NEED TO REMOVE NUMERICALS\n",
    "documents_no_stop_no_numeric = remove_numerical(documents_no_stop)\n",
    "#print ('remove numerics: ',documents_no_stop_no_numeric[tester], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmattizing tokens (better than stemming by taking word context into account)\n",
    "documents_no_stop_no_numeric_lemmatize = [[lemmatizer.lemmatize(token) for token in text] \n",
    "                                                    for text in documents_no_stop_no_numeric]\n",
    "\n",
    "#print ('lemmatize: ',documents_no_stop_no_numeric_lemmatize[tester], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-english words\n",
    "documents_no_stop_no_numeric_lemmatize_english = [[token for token in text if (eng_dic.check(token)) ] \n",
    "                                                            for text in documents_no_stop_no_numeric_lemmatize]\n",
    "\n",
    "#print ('no english: ',documents_no_stop_no_numeric_lemmatize_english[tester], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ready corpus\n",
    "df['reviewCleaned'] = [\" \".join(doc) for doc in documents_no_stop_no_numeric_lemmatize_english] \n",
    "\n",
    "#print (df['reviewCleaned'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_neg_pos = []\n",
    "for row in df['reviewCleaned']:\n",
    "    review = []\n",
    "    new_word = \"\"\n",
    "    for word in row.split(\" \"):\n",
    "        if word in NEGATIONWORDS:\n",
    "            new_word = word\n",
    "        if word in POSITIVEWORDS:\n",
    "            if not new_word:\n",
    "                review.append(word)\n",
    "            else:\n",
    "                new_word = new_word + word\n",
    "                review.append(new_word)\n",
    "                new_word = \"\"\n",
    "        if word in NEGATIVEWORDS:\n",
    "            review.append(word)\n",
    "    document_neg_pos.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ready corpus\n",
    "df[\"reviewWorded\"] = [\" \".join(doc) for doc in document_neg_pos]\n",
    "\n",
    "#print (df[\"reviewWorded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.4,min_df=2)\n",
    "\n",
    "# fit vectorizer, carry out vectorization and display results\n",
    "vectorizer.fit(df['reviewWorded'])\n",
    "documents_vec = vectorizer.transform(df['reviewWorded'])\n",
    "#print(documents_vec) # sparse matrix representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=documents_vec.toarray()\n",
    "y=df['overall']\n",
    "categories=np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fayezmourad/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "X_scaled=[]\n",
    "for doc in X:\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_scaled.append(np.ravel(min_max_scaler.fit_transform(doc.reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled[0][X_scaled[0] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy arrays\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select k best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k should be >=0, <= n_features; got 3244.Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c307459d6dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m parameter_search_feature_selection(LogisticRegression(),SelectKBest(),'k',np.logspace(2,3.7,10).astype(int),\n\u001b[0;32m----> 3\u001b[0;31m                                    X_train,y_train,random_state=random_state)\n\u001b[0m",
      "\u001b[0;32m/Users/fayezmourad/Desktop/project-pace-mourad/fayez/src/CV.py\u001b[0m in \u001b[0;36mparameter_search_feature_selection\u001b[0;34m(estimator, feature_selector, param_name, param_range, X, y, cv, scoring, random_state)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mfeature_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mX_train_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mX_test_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Trying it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fayezmourad/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fayezmourad/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m                             % (self.score_func, type(self.score_func)))\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fayezmourad/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    472\u001b[0m             raise ValueError(\"k should be >=0, <= n_features; got %r.\"\n\u001b[1;32m    473\u001b[0m                              \u001b[0;34m\"Use k='all' to return all features.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                              % self.k)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k should be >=0, <= n_features; got 3244.Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "parameter_search_feature_selection(LogisticRegression(),SelectKBest(),'k',np.logspace(2,3.7,10).astype(int),\n",
    "                                   X_train,y_train,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_concepts,X_test_concepts = LSI_concepts(X_train,X_test,num_lsi_topics = 100,words=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train,categories)\n",
    "print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "accuracy=[]\n",
    "num_lsi_topics_array = [25,30,35,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,6000,7000]\n",
    "# Check various C\n",
    "for num_lsi_topics in num_lsi_topics_array:\n",
    "    print(num_lsi_topics)\n",
    "    # fit classifier \n",
    "    X_train_concepts,X_test_concepts = LSI_concepts(X_train,X_test,num_lsi_topics,words=vectorizer.get_feature_names())\n",
    "    \n",
    "    y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train, categories)\n",
    "    print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    accuracy.append(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# Plot\n",
    "plt.plot(num_lsi_topics_array,accuracy)\n",
    "plt.ylabel(\"accuracy Score\")\n",
    "plt.xlabel(\"number of topics values\")\n",
    "plt.show()\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))\n",
    "best_c=num_lsi_topics_array[np.argmax(accuracy)]\n",
    "print(\"best num_lsi_topics=\",best_c,\"with accuracy score:\", np.max(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE LSI number of concepts and then tune update every and passes\n",
    "start = time.time()\n",
    "X_train_concepts,X_test_concepts = LDA_concepts(X_train,X_test,num_lda_topics = 100,words=vectorizer.get_feature_names())\n",
    "y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train,categories)\n",
    "print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "accuracy=[]\n",
    "num_lda_topics_array = [25,30,35,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,6000,7000]\n",
    "# Check various C\n",
    "for num_lda_topics in num_lda_topics_array:\n",
    "    print(num_lda_topics)\n",
    "    # fit classifier \n",
    "    X_train_concepts,X_test_concepts = LDA_concepts(X_train,X_test,num_lsi_topics,words=vectorizer.get_feature_names())\n",
    "    \n",
    "    y_pred=cosine_pred(X_train_concepts,X_test_concepts,y_train, categories)\n",
    "    print(\"accuracy:\",accuracy_score(y_test,y_pred))\n",
    "    accuracy.append(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# Plot\n",
    "plt.plot(num_lda_topics_array,accuracy)\n",
    "plt.ylabel(\"accuracy Score\")\n",
    "plt.xlabel(\"Number of topics values\")\n",
    "plt.show()\n",
    "\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))\n",
    "best_c=num_lsi_topics_array[np.argmax(accuracy)]\n",
    "print(\"best num_lsi_topics=\",best_c,\"with accuracy score:\", np.max(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dummy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model: Assign the categories randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.357\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dmy=DummyClassifier(random_state=random_state)\n",
    "dmy.fit(X_train,y_train)\n",
    "y_pred=dmy.predict(X_test)\n",
    "print(\"accuracy:\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category  we regroup the documents belonging to this category together. Then for each element we want to predict, we check how similar it is compared to each group (cosine similarity). The most similar group gives the category to the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "[CV] ................................................. , total= 1.5min\n",
      "[CV]  ................................................................\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "[CV] ................................................. , total= 1.6min\n",
      "[CV]  ................................................................\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "[CV] ................................................. , total= 1.7min\n",
      "[CV]  ................................................................\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "[CV] ................................................. , total= 1.6min\n",
      "[CV]  ................................................................\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "[CV] ................................................. , total= 1.6min\n",
      "Mean CV valdiation score: 0.0\n",
      "Execution time : 477.7514579296112 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.0min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "start= time.time()\n",
    "\n",
    "start= time.time()\n",
    "cosine_classifier=Cosine_sim(categories)\n",
    "print(\"Mean CV valdiation score:\",np.mean(cross_val_score(\n",
    "    cosine_classifier,X_train,y_train,scoring='accuracy',cv=5,verbose=2)))\n",
    "end=time.time()\n",
    "print(\"Execution time : {} seconds\".format(end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try various maximum depths for a classic decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_search(DecisionTreeClassifier(random_state=random_state),\n",
    "                {'max_depth':range(1,200,10)},\n",
    "                X_train,\n",
    "                y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "parameter_search(KNeighborsClassifier(),\n",
    "                {'n_neighbors':range(1,50,3)},\n",
    "                X_train,\n",
    "                y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_search(SVC(),\n",
    "                {'C':np.logspace(-5,5,11)},\n",
    "                X_train,\n",
    "                y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_search(LogisticRegression(),\n",
    "                {'C':np.logspace(-5,5,11)},\n",
    "                X_train,\n",
    "                y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
